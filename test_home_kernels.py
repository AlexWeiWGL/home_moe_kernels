#!/usr/bin/env python3
"""
HoME CUDA Kernels ä¸“ç”¨æµ‹è¯•æ–‡ä»¶

è¿™ä¸ªæ–‡ä»¶ä¸“é—¨æµ‹è¯• home_kernels.cu ä¸­å®ç°çš„æ‰€æœ‰CUDAç®—å­ï¼š
1. home_expert_forward - ä¸“å®¶ç½‘ç»œå‰å‘ä¼ æ’­
2. lora_gate_forward - LoRAé—¨æ§å‰å‘ä¼ æ’­  
3. gate_weights_forward - é—¨æ§æƒé‡å‰å‘ä¼ æ’­
4. expert_weighted_sum_forward - ä¸“å®¶åŠ æƒæ±‚å’Œ
5. fused_batch_norm_silu_forward - èåˆBatchNorm+SiLUç®—å­

æ¯ä¸ªç®—å­éƒ½åŒ…å«ï¼š
- åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯
- è¾“å‡ºå½¢çŠ¶éªŒè¯
- æ•°å€¼ç²¾åº¦éªŒè¯ï¼ˆä¸PyTorchå®ç°å¯¹æ¯”ï¼‰
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- å¤šç§æµ‹è¯•ç”¨ä¾‹è¦†ç›–

ä½¿ç”¨æ–¹æ³•:
    python test_home_kernels.py                    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    python test_home_kernels.py --test expert      # åªæµ‹è¯•ä¸“å®¶ç½‘ç»œ
    python test_home_kernels.py --test lora        # åªæµ‹è¯•LoRAé—¨æ§
    python test_home_kernels.py --test gate        # åªæµ‹è¯•é—¨æ§æƒé‡
    python test_home_kernels.py --test weighted    # åªæµ‹è¯•åŠ æƒæ±‚å’Œ
    python test_home_kernels.py --test fused       # åªæµ‹è¯•èåˆç®—å­
    python test_home_kernels.py --quick            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
"""

import torch
import torch.nn as nn
import time
import sys
import os
import argparse
import numpy as np
from typing import List, Dict, Tuple, Optional

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å°è¯•å¯¼å…¥CUDA kernelæ¨¡å—
try:
    import home_kernels
    CUDA_KERNELS_AVAILABLE = True
    print("âœ“ CUDA kernelsæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸  CUDA kernelsæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å…ˆç¼–è¯‘CUDA kernels: python setup_home.py build_ext --inplace")
    CUDA_KERNELS_AVAILABLE = False
    home_kernels = None


class HomeKernelTester:
    """HoME CUDA Kernels æµ‹è¯•å™¨"""
    
    def __init__(self, device: str = 'cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.test_results = {}
        
    def print_gpu_memory(self):
        """æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"GPUå†…å­˜ - å·²åˆ†é…: {allocated:.2f} GB, å·²ç¼“å­˜: {reserved:.2f} GB")
            return allocated, reserved
        return 0, 0
    
    def clear_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def run_performance_test(self, kernel_func, num_runs: int = 10, warmup_runs: int = 3) -> Dict:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        times = []
        
        # é¢„çƒ­
        for _ in range(warmup_runs):
            with torch.no_grad():
                _ = kernel_func()
        
        # æµ‹è¯•
        for _ in range(num_runs):
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            start_time = time.time()
            
            with torch.no_grad():
                _ = kernel_func()
            
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        std_time = (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5
        
        return {
            'avg_time': avg_time,
            'min_time': min_time,
            'max_time': max_time,
            'std_time': std_time,
            'times': times
        }
    
    def test_home_expert_forward(self):
        """æµ‹è¯•home_expert_forwardç®—å­"""
        print("\n" + "=" * 80)
        print("æµ‹è¯• home_expert_forward ç®—å­")
        print("=" * 80)
        
        if not CUDA_KERNELS_AVAILABLE:
            print("âš ï¸  CUDA kernelsä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {"batch_size": 32, "input_dim": 256, "hidden_dim": 256, "num_experts": 5},
            {"batch_size": 64, "input_dim": 512, "hidden_dim": 512, "num_experts": 8},
            {"batch_size": 128, "input_dim": 128, "hidden_dim": 128, "num_experts": 3},
            {"batch_size": 4096, "input_dim": 700, "hidden_dim": 256, "num_experts": 5},  # é«˜ååé‡æµ‹è¯•
        ]
        
        for i, case in enumerate(test_cases):
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {case}")
            print("-" * 60)
            
            batch_size = case["batch_size"]
            input_dim = case["input_dim"]
            hidden_dim = case["hidden_dim"]
            num_experts = case["num_experts"]
            
            # é«˜ååé‡æµ‹è¯•ç”¨ä¾‹ç‰¹æ®Šå¤„ç†
            if batch_size >= 4096:
                print(f"  ğŸš€ é«˜ååé‡æµ‹è¯•ç”¨ä¾‹")
                self.print_gpu_memory()
            
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                torch.manual_seed(42)
                input_data = torch.randn(batch_size, input_dim, device=self.device)
                expert_weights = torch.randn(num_experts, input_dim, hidden_dim, device=self.device)
                expert_biases = torch.randn(num_experts, hidden_dim, device=self.device)
                expert_indices = torch.arange(num_experts, dtype=torch.int32, device=self.device)
                
                # æµ‹è¯•CUDA kernel
                with torch.no_grad():
                    cuda_output = home_kernels.home_expert_forward(
                        input_data, expert_weights, expert_biases, expert_indices, num_experts, True
                    )
                
                print(f"âœ“ CUDA kernelæ‰§è¡ŒæˆåŠŸ")
                print(f"  è¾“å‡ºå½¢çŠ¶: {cuda_output.shape}")
                print(f"  è¾“å‡ºæ•°æ®ç±»å‹: {cuda_output.dtype}")
                print(f"  è¾“å‡ºè®¾å¤‡: {cuda_output.device}")
                
                # éªŒè¯è¾“å‡ºå½¢çŠ¶
                expected_shape = (batch_size, num_experts, hidden_dim)
                if cuda_output.shape == expected_shape:
                    print(f"âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {cuda_output.shape}")
                else:
                    print(f"âœ— è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å®é™… {cuda_output.shape}")
                    continue
                
                # æ€§èƒ½æµ‹è¯•
                def kernel_func():
                    return home_kernels.home_expert_forward(
                        input_data, expert_weights, expert_biases, expert_indices, num_experts, True
                    )
                
                perf_stats = self.run_performance_test(kernel_func)
                print(f"  æ€§èƒ½ç»Ÿè®¡:")
                print(f"    å¹³å‡æ—¶é—´: {perf_stats['avg_time']:.4f} ms")
                print(f"    æœ€å°æ—¶é—´: {perf_stats['min_time']:.4f} ms")
                print(f"    æœ€å¤§æ—¶é—´: {perf_stats['max_time']:.4f} ms")
                print(f"    æ ‡å‡†å·®: {perf_stats['std_time']:.4f} ms")
                
                # é«˜ååé‡æµ‹è¯•ç”¨ä¾‹é¢å¤–è®¡ç®—ååé‡
                if batch_size >= 4096:
                    throughput = batch_size / (perf_stats['avg_time'] / 1000)
                    print(f"    ååé‡: {throughput:.0f} samples/sec")
                    self.print_gpu_memory()
                
                # æ¸…ç†å†…å­˜
                del input_data, expert_weights, expert_biases, expert_indices, cuda_output
                self.clear_gpu_memory()
                
            except Exception as e:
                print(f"âœ— CUDA kernelæ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        print("\nâœ“ home_expert_forward ç®—å­æµ‹è¯•å®Œæˆ!")
    
    def test_lora_gate_forward(self):
        """æµ‹è¯•lora_gate_forwardç®—å­"""
        print("\n" + "=" * 80)
        print("æµ‹è¯• lora_gate_forward ç®—å­")
        print("=" * 80)
        
        if not CUDA_KERNELS_AVAILABLE:
            print("âš ï¸  CUDA kernelsä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {"batch_size": 32, "input_dim": 256, "rank": 16, "output_dim": 256},
            {"batch_size": 64, "input_dim": 512, "rank": 32, "output_dim": 512},
            {"batch_size": 128, "input_dim": 128, "rank": 8, "output_dim": 128},
            {"batch_size": 4096, "input_dim": 700, "rank": 16, "output_dim": 700},  # é«˜ååé‡æµ‹è¯•
        ]
        
        for i, case in enumerate(test_cases):
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {case}")
            print("-" * 60)
            
            batch_size = case["batch_size"]
            input_dim = case["input_dim"]
            rank = case["rank"]
            output_dim = case["output_dim"]
            
            # é«˜ååé‡æµ‹è¯•ç”¨ä¾‹ç‰¹æ®Šå¤„ç†
            if batch_size >= 4096:
                print(f"  ğŸš€ é«˜ååé‡æµ‹è¯•ç”¨ä¾‹")
                self.print_gpu_memory()
            
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                torch.manual_seed(42)
                input_data = torch.randn(batch_size, input_dim, device=self.device)
                A_matrix = torch.randn(input_dim, rank, device=self.device)
                B_matrix = torch.randn(rank, output_dim, device=self.device)
                
                # æµ‹è¯•CUDA kernel
                with torch.no_grad():
                    cuda_output = home_kernels.lora_gate_forward(
                        input_data, A_matrix, B_matrix, True  # use_vectorized=True
                    )
                
                print(f"âœ“ CUDA kernelæ‰§è¡ŒæˆåŠŸ")
                print(f"  è¾“å‡ºå½¢çŠ¶: {cuda_output.shape}")
                print(f"  è¾“å‡ºæ•°æ®ç±»å‹: {cuda_output.dtype}")
                print(f"  è¾“å‡ºè®¾å¤‡: {cuda_output.device}")
                
                # éªŒè¯è¾“å‡ºå½¢çŠ¶
                expected_shape = (batch_size, output_dim)
                if cuda_output.shape == expected_shape:
                    print(f"âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {cuda_output.shape}")
                else:
                    print(f"âœ— è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å®é™… {cuda_output.shape}")
                    continue
                
                # ä¸PyTorchå®ç°å¯¹æ¯”éªŒè¯
                with torch.no_grad():
                    pytorch_output = torch.matmul(torch.matmul(input_data, A_matrix), B_matrix)
                
                # è®¡ç®—æ•°å€¼å·®å¼‚
                diff = torch.abs(cuda_output - pytorch_output)
                max_diff = torch.max(diff).item()
                mean_diff = torch.mean(diff).item()
                
                print(f"  ä¸PyTorchå®ç°å¯¹æ¯”:")
                print(f"    æœ€å¤§å·®å¼‚: {max_diff:.6f}")
                print(f"    å¹³å‡å·®å¼‚: {mean_diff:.6f}")
                
                if max_diff < 1e-4:
                    print(f"âœ“ æ•°å€¼ç²¾åº¦éªŒè¯é€šè¿‡")
                else:
                    print(f"âš ï¸  æ•°å€¼ç²¾åº¦å¯èƒ½å­˜åœ¨å·®å¼‚")
                
                # æ€§èƒ½æµ‹è¯•
                def kernel_func():
                    return home_kernels.lora_gate_forward(input_data, A_matrix, B_matrix, True)
                
                perf_stats = self.run_performance_test(kernel_func)
                print(f"  æ€§èƒ½ç»Ÿè®¡:")
                print(f"    å¹³å‡æ—¶é—´: {perf_stats['avg_time']:.4f} ms")
                print(f"    æœ€å°æ—¶é—´: {perf_stats['min_time']:.4f} ms")
                print(f"    æœ€å¤§æ—¶é—´: {perf_stats['max_time']:.4f} ms")
                print(f"    æ ‡å‡†å·®: {perf_stats['std_time']:.4f} ms")
                
                # é«˜ååé‡æµ‹è¯•ç”¨ä¾‹é¢å¤–è®¡ç®—ååé‡
                if batch_size >= 4096:
                    throughput = batch_size / (perf_stats['avg_time'] / 1000)
                    print(f"    ååé‡: {throughput:.0f} samples/sec")
                    self.print_gpu_memory()
                
                # æ¸…ç†å†…å­˜
                del input_data, A_matrix, B_matrix, cuda_output, pytorch_output
                self.clear_gpu_memory()
                
            except Exception as e:
                print(f"âœ— CUDA kernelæ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        print("\nâœ“ lora_gate_forward ç®—å­æµ‹è¯•å®Œæˆ!")
    
    def test_gate_weights_forward(self):
        """æµ‹è¯•gate_weights_forwardç®—å­"""
        print("\n" + "=" * 80)
        print("æµ‹è¯• gate_weights_forward ç®—å­")
        print("=" * 80)
        
        if not CUDA_KERNELS_AVAILABLE:
            print("âš ï¸  CUDA kernelsä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {"batch_size": 32, "gate_dim": 256, "num_experts": 5, "use_softmax": True},
            {"batch_size": 64, "gate_dim": 512, "num_experts": 8, "use_softmax": True},
            {"batch_size": 128, "gate_dim": 128, "num_experts": 3, "use_softmax": False},
            {"batch_size": 4096, "gate_dim": 700, "num_experts": 5, "use_softmax": True},  # é«˜ååé‡æµ‹è¯•
        ]
        
        for i, case in enumerate(test_cases):
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {case}")
            print("-" * 60)
            
            batch_size = case["batch_size"]
            gate_dim = case["gate_dim"]
            num_experts = case["num_experts"]
            use_softmax = case["use_softmax"]
            
            # é«˜ååé‡æµ‹è¯•ç”¨ä¾‹ç‰¹æ®Šå¤„ç†
            if batch_size >= 4096:
                print(f"  ğŸš€ é«˜ååé‡æµ‹è¯•ç”¨ä¾‹")
                self.print_gpu_memory()
            
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                torch.manual_seed(42)
                gate_states = torch.randn(batch_size, gate_dim, device=self.device)
                gate_weights = torch.randn(gate_dim, num_experts, device=self.device)
                
                # æµ‹è¯•CUDA kernel
                with torch.no_grad():
                    cuda_output = home_kernels.gate_weights_forward(
                        gate_states, gate_weights, use_softmax
                    )
                
                print(f"âœ“ CUDA kernelæ‰§è¡ŒæˆåŠŸ")
                print(f"  è¾“å‡ºå½¢çŠ¶: {cuda_output.shape}")
                print(f"  è¾“å‡ºæ•°æ®ç±»å‹: {cuda_output.dtype}")
                print(f"  è¾“å‡ºè®¾å¤‡: {cuda_output.device}")
                
                # éªŒè¯è¾“å‡ºå½¢çŠ¶
                expected_shape = (batch_size, num_experts)
                if cuda_output.shape == expected_shape:
                    print(f"âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {cuda_output.shape}")
                else:
                    print(f"âœ— è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å®é™… {cuda_output.shape}")
                    continue
                
                # ä¸PyTorchå®ç°å¯¹æ¯”éªŒè¯
                with torch.no_grad():
                    pytorch_output = torch.matmul(gate_states, gate_weights)
                    if use_softmax:
                        pytorch_output = torch.softmax(pytorch_output, dim=-1)
                
                # è®¡ç®—æ•°å€¼å·®å¼‚
                diff = torch.abs(cuda_output - pytorch_output)
                max_diff = torch.max(diff).item()
                mean_diff = torch.mean(diff).item()
                
                print(f"  ä¸PyTorchå®ç°å¯¹æ¯”:")
                print(f"    æœ€å¤§å·®å¼‚: {max_diff:.6f}")
                print(f"    å¹³å‡å·®å¼‚: {mean_diff:.6f}")
                
                if max_diff < 1e-4:
                    print(f"âœ“ æ•°å€¼ç²¾åº¦éªŒè¯é€šè¿‡")
                else:
                    print(f"âš ï¸  æ•°å€¼ç²¾åº¦å¯èƒ½å­˜åœ¨å·®å¼‚")
                
                # éªŒè¯softmaxç‰¹æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if use_softmax:
                    row_sums = torch.sum(cuda_output, dim=-1)
                    print(f"  SoftmaxéªŒè¯:")
                    print(f"    è¡Œå’ŒèŒƒå›´: [{torch.min(row_sums).item():.6f}, {torch.max(row_sums).item():.6f}]")
                    if torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-5):
                        print(f"    âœ“ Softmaxå½’ä¸€åŒ–æ­£ç¡®")
                    else:
                        print(f"    âœ— Softmaxå½’ä¸€åŒ–å¯èƒ½æœ‰é—®é¢˜")
                
                # æ€§èƒ½æµ‹è¯•
                def kernel_func():
                    return home_kernels.gate_weights_forward(gate_states, gate_weights, use_softmax)
                
                perf_stats = self.run_performance_test(kernel_func)
                print(f"  æ€§èƒ½ç»Ÿè®¡:")
                print(f"    å¹³å‡æ—¶é—´: {perf_stats['avg_time']:.4f} ms")
                print(f"    æœ€å°æ—¶é—´: {perf_stats['min_time']:.4f} ms")
                print(f"    æœ€å¤§æ—¶é—´: {perf_stats['max_time']:.4f} ms")
                print(f"    æ ‡å‡†å·®: {perf_stats['std_time']:.4f} ms")
                
                # é«˜ååé‡æµ‹è¯•ç”¨ä¾‹é¢å¤–è®¡ç®—ååé‡
                if batch_size >= 4096:
                    throughput = batch_size / (perf_stats['avg_time'] / 1000)
                    print(f"    ååé‡: {throughput:.0f} samples/sec")
                    self.print_gpu_memory()
                
                # æ¸…ç†å†…å­˜
                del gate_states, gate_weights, cuda_output, pytorch_output
                self.clear_gpu_memory()
                
            except Exception as e:
                print(f"âœ— CUDA kernelæ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        print("\nâœ“ gate_weights_forward ç®—å­æµ‹è¯•å®Œæˆ!")
    
    def test_expert_weighted_sum_forward(self):
        """æµ‹è¯•expert_weighted_sum_forwardç®—å­"""
        print("\n" + "=" * 80)
        print("æµ‹è¯• expert_weighted_sum_forward ç®—å­")
        print("=" * 80)
        
        if not CUDA_KERNELS_AVAILABLE:
            print("âš ï¸  CUDA kernelsä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {"batch_size": 32, "hidden_dim": 256, "num_experts": 5},
            {"batch_size": 64, "hidden_dim": 512, "num_experts": 8},
            {"batch_size": 128, "hidden_dim": 128, "num_experts": 3},
            {"batch_size": 4096, "hidden_dim": 256, "num_experts": 5},  # é«˜ååé‡æµ‹è¯•
        ]
        
        for i, case in enumerate(test_cases):
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {case}")
            print("-" * 60)
            
            batch_size = case["batch_size"]
            hidden_dim = case["hidden_dim"]
            num_experts = case["num_experts"]
            
            # é«˜ååé‡æµ‹è¯•ç”¨ä¾‹ç‰¹æ®Šå¤„ç†
            if batch_size >= 4096:
                print(f"  ğŸš€ é«˜ååé‡æµ‹è¯•ç”¨ä¾‹")
                self.print_gpu_memory()
            
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                torch.manual_seed(42)
                expert_outputs = torch.randn(batch_size, hidden_dim, num_experts, device=self.device)
                gate_weights = torch.randn(batch_size, num_experts, device=self.device)
                
                # æµ‹è¯•CUDA kernel
                with torch.no_grad():
                    cuda_output = home_kernels.expert_weighted_sum_forward(
                        expert_outputs, gate_weights
                    )
                
                print(f"âœ“ CUDA kernelæ‰§è¡ŒæˆåŠŸ")
                print(f"  è¾“å‡ºå½¢çŠ¶: {cuda_output.shape}")
                print(f"  è¾“å‡ºæ•°æ®ç±»å‹: {cuda_output.dtype}")
                print(f"  è¾“å‡ºè®¾å¤‡: {cuda_output.device}")
                
                # éªŒè¯è¾“å‡ºå½¢çŠ¶
                expected_shape = (batch_size, hidden_dim)
                if cuda_output.shape == expected_shape:
                    print(f"âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {cuda_output.shape}")
                else:
                    print(f"âœ— è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å®é™… {cuda_output.shape}")
                    continue
                
                # ä¸PyTorchå®ç°å¯¹æ¯”éªŒè¯
                with torch.no_grad():
                    # æ‰©å±•gate_weightsç»´åº¦ä»¥åŒ¹é…expert_outputs
                    gate_weights_expanded = gate_weights.unsqueeze(1)  # [batch_size, 1, num_experts]
                    pytorch_output = torch.sum(expert_outputs * gate_weights_expanded, dim=2)
                
                # è®¡ç®—æ•°å€¼å·®å¼‚
                diff = torch.abs(cuda_output - pytorch_output)
                max_diff = torch.max(diff).item()
                mean_diff = torch.mean(diff).item()
                
                print(f"  ä¸PyTorchå®ç°å¯¹æ¯”:")
                print(f"    æœ€å¤§å·®å¼‚: {max_diff:.6f}")
                print(f"    å¹³å‡å·®å¼‚: {mean_diff:.6f}")
                
                if max_diff < 1e-4:
                    print(f"âœ“ æ•°å€¼ç²¾åº¦éªŒè¯é€šè¿‡")
                else:
                    print(f"âš ï¸  æ•°å€¼ç²¾åº¦å¯èƒ½å­˜åœ¨å·®å¼‚")
                
                # æ€§èƒ½æµ‹è¯•
                def kernel_func():
                    return home_kernels.expert_weighted_sum_forward(expert_outputs, gate_weights)
                
                perf_stats = self.run_performance_test(kernel_func)
                print(f"  æ€§èƒ½ç»Ÿè®¡:")
                print(f"    å¹³å‡æ—¶é—´: {perf_stats['avg_time']:.4f} ms")
                print(f"    æœ€å°æ—¶é—´: {perf_stats['min_time']:.4f} ms")
                print(f"    æœ€å¤§æ—¶é—´: {perf_stats['max_time']:.4f} ms")
                print(f"    æ ‡å‡†å·®: {perf_stats['std_time']:.4f} ms")
                
                # é«˜ååé‡æµ‹è¯•ç”¨ä¾‹é¢å¤–è®¡ç®—ååé‡
                if batch_size >= 4096:
                    throughput = batch_size / (perf_stats['avg_time'] / 1000)
                    print(f"    ååé‡: {throughput:.0f} samples/sec")
                    self.print_gpu_memory()
                
                # æ¸…ç†å†…å­˜
                del expert_outputs, gate_weights, cuda_output, pytorch_output
                self.clear_gpu_memory()
                
            except Exception as e:
                print(f"âœ— CUDA kernelæ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        print("\nâœ“ expert_weighted_sum_forward ç®—å­æµ‹è¯•å®Œæˆ!")
    
    def test_fused_batch_norm_silu_forward(self):
        """æµ‹è¯•fused_batch_norm_silu_forwardç®—å­"""
        print("\n" + "=" * 80)
        print("æµ‹è¯• fused_batch_norm_silu_forward ç®—å­")
        print("=" * 80)
        
        if not CUDA_KERNELS_AVAILABLE:
            print("âš ï¸  CUDA kernelsä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {"batch_size": 32, "num_experts": 5, "hidden_dim": 256},
            {"batch_size": 64, "num_experts": 8, "hidden_dim": 512},
            {"batch_size": 128, "num_experts": 3, "hidden_dim": 128},
            {"batch_size": 4096, "num_experts": 5, "hidden_dim": 256},  # é«˜ååé‡æµ‹è¯•
        ]
        
        for i, case in enumerate(test_cases):
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {case}")
            print("-" * 60)
            
            batch_size = case["batch_size"]
            num_experts = case["num_experts"]
            hidden_dim = case["hidden_dim"]
            
            # é«˜ååé‡æµ‹è¯•ç”¨ä¾‹ç‰¹æ®Šå¤„ç†
            if batch_size >= 4096:
                print(f"  ğŸš€ é«˜ååé‡æµ‹è¯•ç”¨ä¾‹")
                self.print_gpu_memory()
            
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                torch.manual_seed(42)
                data = torch.randn(batch_size, num_experts, hidden_dim, device=self.device)
                bn_weights = torch.randn(num_experts, hidden_dim, device=self.device)
                bn_biases = torch.randn(num_experts, hidden_dim, device=self.device)
                running_mean = torch.randn(num_experts, hidden_dim, device=self.device)
                running_var = torch.ones(num_experts, hidden_dim, device=self.device)  # ç¡®ä¿æ–¹å·®ä¸ºæ­£
                epsilon = 1e-5
                
                # æµ‹è¯•CUDA kernel
                with torch.no_grad():
                    cuda_output = home_kernels.fused_batch_norm_silu_forward(
                        data, bn_weights, bn_biases, running_mean, running_var, epsilon
                    )
                
                print(f"âœ“ CUDA kernelæ‰§è¡ŒæˆåŠŸ")
                print(f"  è¾“å‡ºå½¢çŠ¶: {cuda_output.shape}")
                print(f"  è¾“å‡ºæ•°æ®ç±»å‹: {cuda_output.dtype}")
                print(f"  è¾“å‡ºè®¾å¤‡: {cuda_output.device}")
                
                # éªŒè¯è¾“å‡ºå½¢çŠ¶
                expected_shape = (batch_size, num_experts, hidden_dim)
                if cuda_output.shape == expected_shape:
                    print(f"âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {cuda_output.shape}")
                else:
                    print(f"âœ— è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ› {expected_shape}, å®é™… {cuda_output.shape}")
                    continue
                
                # ä¸PyTorchå®ç°å¯¹æ¯”éªŒè¯
                with torch.no_grad():
                    # æ‰‹åŠ¨å®ç°BatchNorm + SiLU
                    # é¦–å…ˆåº”ç”¨BatchNorm
                    normalized = (data - running_mean.unsqueeze(0)) / torch.sqrt(running_var.unsqueeze(0) + epsilon)
                    batch_norm_output = normalized * bn_weights.unsqueeze(0) + bn_biases.unsqueeze(0)
                    
                    # ç„¶ååº”ç”¨SiLUæ¿€æ´»å‡½æ•°
                    pytorch_output = batch_norm_output * torch.sigmoid(batch_norm_output)
                
                # è®¡ç®—æ•°å€¼å·®å¼‚
                diff = torch.abs(cuda_output - pytorch_output)
                max_diff = torch.max(diff).item()
                mean_diff = torch.mean(diff).item()
                
                print(f"  ä¸PyTorchå®ç°å¯¹æ¯”:")
                print(f"    æœ€å¤§å·®å¼‚: {max_diff:.6f}")
                print(f"    å¹³å‡å·®å¼‚: {mean_diff:.6f}")
                
                if max_diff < 1e-4:
                    print(f"âœ“ æ•°å€¼ç²¾åº¦éªŒè¯é€šè¿‡")
                else:
                    print(f"âš ï¸  æ•°å€¼ç²¾åº¦å¯èƒ½å­˜åœ¨å·®å¼‚")
                
                # éªŒè¯SiLUæ¿€æ´»å‡½æ•°ç‰¹æ€§
                # SiLU(x) = x * sigmoid(x)ï¼Œåº”è¯¥ä¿æŒè¾“å…¥çš„æ­£è´Ÿæ€§
                input_positive = data > 0
                output_positive = cuda_output > 0
                consistency = torch.all(input_positive == output_positive)
                
                print(f"  SiLUæ¿€æ´»å‡½æ•°éªŒè¯:")
                print(f"    æ­£è´Ÿæ€§ä¸€è‡´æ€§: {'âœ“' if consistency else 'âœ—'}")
                
                # æ€§èƒ½æµ‹è¯•
                def kernel_func():
                    return home_kernels.fused_batch_norm_silu_forward(
                        data, bn_weights, bn_biases, running_mean, running_var, epsilon
                    )
                
                perf_stats = self.run_performance_test(kernel_func)
                print(f"  æ€§èƒ½ç»Ÿè®¡:")
                print(f"    å¹³å‡æ—¶é—´: {perf_stats['avg_time']:.4f} ms")
                print(f"    æœ€å°æ—¶é—´: {perf_stats['min_time']:.4f} ms")
                print(f"    æœ€å¤§æ—¶é—´: {perf_stats['max_time']:.4f} ms")
                print(f"    æ ‡å‡†å·®: {perf_stats['std_time']:.4f} ms")
                
                # é«˜ååé‡æµ‹è¯•ç”¨ä¾‹é¢å¤–è®¡ç®—ååé‡
                if batch_size >= 4096:
                    throughput = batch_size / (perf_stats['avg_time'] / 1000)
                    print(f"    ååé‡: {throughput:.0f} samples/sec")
                    self.print_gpu_memory()
                
                # æ¸…ç†å†…å­˜
                del data, bn_weights, bn_biases, running_mean, running_var, cuda_output, pytorch_output
                self.clear_gpu_memory()
                
            except Exception as e:
                print(f"âœ— CUDA kernelæ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        print("\nâœ“ fused_batch_norm_silu_forward ç®—å­æµ‹è¯•å®Œæˆ!")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("HoME CUDA Kernels å®Œæ•´æµ‹è¯•å¥—ä»¶")
        print("=" * 80)
        
        # æ£€æŸ¥CUDAå¯ç”¨æ€§
        if torch.cuda.is_available():
            print(f"CUDAå¯ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            print("è­¦å‘Š: CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿è¡Œ")
        
        # è¿è¡Œæ‰€æœ‰ç®—å­æµ‹è¯•
        self.test_home_expert_forward()
        self.test_lora_gate_forward()
        self.test_gate_weights_forward()
        self.test_expert_weighted_sum_forward()
        self.test_fused_batch_norm_silu_forward()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ æ‰€æœ‰CUDA kernelæµ‹è¯•å®Œæˆ! ğŸ‰")
        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='HoME CUDA Kernels æµ‹è¯•å·¥å…·')
    parser.add_argument('--test', choices=['all', 'expert', 'lora', 'gate', 'weighted', 'fused'], 
                       default='all', help='é€‰æ‹©è¦è¿è¡Œçš„æµ‹è¯•ç±»å‹')
    parser.add_argument('--quick', action='store_true', help='è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆå‡å°‘è¿è¡Œæ¬¡æ•°ï¼‰')
    parser.add_argument('--device', default='cuda', help='è¿è¡Œè®¾å¤‡ (cuda/cpu)')
    
    args = parser.parse_args()
    
    print("HoME CUDA Kernels æµ‹è¯•å·¥å…·")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = HomeKernelTester(device=args.device)
    
    if args.test == 'all':
        tester.run_all_tests()
    elif args.test == 'expert':
        tester.test_home_expert_forward()
    elif args.test == 'lora':
        tester.test_lora_gate_forward()
    elif args.test == 'gate':
        tester.test_gate_weights_forward()
    elif args.test == 'weighted':
        tester.test_expert_weighted_sum_forward()
    elif args.test == 'fused':
        tester.test_fused_batch_norm_silu_forward()
    
    if args.quick:
        print("\nâš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼å®Œæˆ!")
    else:
        print("\nâœ… å®Œæ•´æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
