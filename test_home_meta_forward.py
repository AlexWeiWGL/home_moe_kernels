#!/usr/bin/env python3
"""
HoME Meta Forward æµ‹è¯•è„šæœ¬
æµ‹è¯•å®Œæ•´çš„ä¸¤å±‚ä¸“å®¶ç½‘ç»œï¼šFC+ReLU -> FC+BN+SiLU
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥MLPå’ŒDenseæ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥å®Œæ•´çš„MLPå’ŒDenseå±‚å®ç°
from MLP import MLPLayer
from Dense import DenseLayer

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

class PyTorchMetaLayer(nn.Module):
    """ä½¿ç”¨å®Œæ•´MLPå’ŒDenseå±‚å®ç°çš„Metaå±‚ï¼Œç”¨äºå¯¹æ¯”æµ‹è¯•"""
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_experts: int):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_experts = num_experts
        
        # ç¬¬ä¸€å±‚ï¼šä½¿ç”¨MLPLayerå®ç° FC + ReLU
        self.experts1 = nn.ModuleList()
        for i in range(num_experts):
            expert = MLPLayer(input_dim, [hidden_dim], activate="relu", name=f"expert1_{i}")
            self.experts1.append(expert)
        
        # ç¬¬äºŒå±‚ï¼šä½¿ç”¨MLPLayerå®ç° FCï¼Œç„¶åæ·»åŠ BatchNorm + SiLU
        self.experts2 = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        for i in range(num_experts):
            expert = MLPLayer(hidden_dim, [output_dim], activate=None, name=f"expert2_{i}")
            batch_norm = nn.BatchNorm1d(output_dim)
            self.experts2.append(expert)
            self.batch_norms.append(batch_norm)
    
    def forward(self, input_tensor):
        """
        Args:
            input_tensor: [batch_size, input_dim]
        Returns:
            output: [batch_size, num_experts, output_dim]
        """
        batch_size = input_tensor.size(0)
        
        # ç¬¬ä¸€å±‚ï¼šMLP (FC + ReLU)
        hidden_outputs = []
        for expert in self.experts1:
            hidden_output = expert(input_tensor)  # [batch_size, hidden_dim]
            hidden_outputs.append(hidden_output)
        
        # ç¬¬äºŒå±‚ï¼šMLP (FC) + BatchNorm + SiLU
        final_outputs = []
        for i, (expert, batch_norm) in enumerate(zip(self.experts2, self.batch_norms)):
            # ä½¿ç”¨å¯¹åº”çš„hiddenè¾“å‡º
            hidden_output = hidden_outputs[i]  # [batch_size, hidden_dim]
            
            # MLP (FC)
            fc_output = expert(hidden_output)  # [batch_size, output_dim]
            
            # BatchNorm + SiLU
            bn_output = batch_norm(fc_output)  # [batch_size, output_dim]
            silu_output = F.silu(bn_output)  # [batch_size, output_dim]
            
            final_outputs.append(silu_output)
        
        # å †å æˆ [batch_size, num_experts, output_dim]
        output = torch.stack(final_outputs, dim=1)
        return output

def test_pytorch_meta_forward():
    """æµ‹è¯•PyTorchå®ç°çš„Meta Forwardæ¥å£"""
    print("=== PyTorch Meta Forward æµ‹è¯• ===")
    
    # å‚æ•°è®¾ç½®
    batch_size = 4096
    input_dim = 1408
    hidden_dim = 704
    output_dim = 504
    num_experts = 10
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # è¾“å…¥æ•°æ®
    input_tensor = torch.randn(batch_size, input_dim, dtype=torch.bfloat16, device=device)
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_tensor.shape}")
    print(f"å‚æ•°è®¾ç½®: batch_size={batch_size}, input_dim={input_dim}, hidden_dim={hidden_dim}, output_dim={output_dim}, num_experts={num_experts}")
    
    try:
        # åˆ›å»ºPyTorchæ¨¡å‹
        pytorch_model = PyTorchMetaLayer(input_dim, hidden_dim, output_dim, num_experts).to(device)
        pytorch_model = pytorch_model.bfloat16()  # è½¬æ¢ä¸ºBF16
        
        # å¯ç”¨torch.compileè¿›è¡Œä¼˜åŒ–
        print("Enabling torch.compile for PyTorch model...")
        pytorch_model = torch.compile(pytorch_model)
        
        # é¢„çƒ­
        for _ in range(5):
            _ = pytorch_model(input_tensor)
        
        # ä½¿ç”¨CUDA eventsè¿›è¡Œç²¾ç¡®è®¡æ—¶
        torch.cuda.synchronize()
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)
        
        start_event.record()
        for _ in range(50):
            output = pytorch_model(input_tensor)
        end_event.record()
        
        torch.cuda.synchronize()
        avg_time = start_event.elapsed_time(end_event) / 50  # å·²ç»æ˜¯æ¯«ç§’
        
        print(f"PyTorch Meta Forward å¹³å‡æ—¶é—´: {avg_time:.4f} ms")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"è¾“å‡ºæ•°æ®ç±»å‹: {output.dtype}")
        print(f"è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (batch_size, num_experts, output_dim)
        assert output.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: æœŸæœ› {expected_shape}, å®é™… {output.shape}"
        
        print("âœ… PyTorch Meta Forward æµ‹è¯•é€šè¿‡!")
        
        return output, pytorch_model
        
    except Exception as e:
        print(f"âŒ PyTorchæµ‹è¯•å¤±è´¥: {e}")
        return None, None

def test_home_meta_forward():
    """æµ‹è¯•HoME Meta Forwardæ¥å£"""
    print("=== HoME Meta Forward æµ‹è¯• ===")
    
    # å‚æ•°è®¾ç½®
    batch_size = 4096
    input_dim = 1408
    hidden_dim = 704
    output_dim = 504
    num_experts = 10
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # è¾“å…¥æ•°æ®
    input_tensor = torch.randn(batch_size, input_dim, dtype=torch.bfloat16, device=device)
    
    # ç¬¬ä¸€å±‚ä¸“å®¶æƒé‡å’Œåç½®
    expert_weights1 = torch.randn(num_experts, input_dim, hidden_dim, dtype=torch.bfloat16, device=device)
    expert_biases1 = torch.randn(num_experts, hidden_dim, dtype=torch.bfloat16, device=device)
    
    # ç¬¬äºŒå±‚ä¸“å®¶æƒé‡å’Œåç½®
    expert_weights2 = torch.randn(num_experts, hidden_dim, output_dim, dtype=torch.bfloat16, device=device)
    expert_biases2 = torch.randn(num_experts, output_dim, dtype=torch.bfloat16, device=device)
    
    # BatchNormå‚æ•°
    bn_gamma = torch.randn(num_experts, output_dim, dtype=torch.float32, device=device) * 0.1 + 1.0
    bn_beta = torch.randn(num_experts, output_dim, dtype=torch.float32, device=device) * 0.1
    running_mean = torch.randn(num_experts, output_dim, dtype=torch.float32, device=device) * 0.1
    running_var = torch.randn(num_experts, output_dim, dtype=torch.float32, device=device) * 0.1 + 1.0
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_tensor.shape}")
    print(f"ç¬¬ä¸€å±‚æƒé‡å½¢çŠ¶: {expert_weights1.shape}")
    print(f"ç¬¬äºŒå±‚æƒé‡å½¢çŠ¶: {expert_weights2.shape}")
    print(f"BatchNormå‚æ•°å½¢çŠ¶: {bn_gamma.shape}")
    
    # å¯¼å…¥HoME kernels
    try:
        import sys
        sys.path.append('/root/home_moe_kernels/build')
        import home_kernels
        
        print("\n=== æµ‹è¯•HoME Meta Forward ===")
        
        # é¢„çƒ­
        for _ in range(5):
            _ = home_kernels.home_meta_forward_bf16(
                input_tensor, expert_weights1, expert_biases1,
                expert_weights2, expert_biases2,
                bn_gamma, bn_beta, running_mean, running_var,
                num_experts, True, 1e-5
            )
        
        # ä½¿ç”¨CUDA eventsè¿›è¡Œç²¾ç¡®è®¡æ—¶
        torch.cuda.synchronize()
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)
        
        start_event.record()
        for _ in range(50):
            output = home_kernels.home_meta_forward_bf16(
                input_tensor, expert_weights1, expert_biases1,
                expert_weights2, expert_biases2,
                bn_gamma, bn_beta, running_mean, running_var,
                num_experts, True, 1e-5
            )
        end_event.record()
        
        torch.cuda.synchronize()
        avg_time = start_event.elapsed_time(end_event) / 50  # å·²ç»æ˜¯æ¯«ç§’
        
        print(f"HoME Meta Forward å¹³å‡æ—¶é—´: {avg_time:.4f} ms")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"è¾“å‡ºæ•°æ®ç±»å‹: {output.dtype}")
        print(f"è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (batch_size, num_experts, output_dim)
        assert output.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: æœŸæœ› {expected_shape}, å®é™… {output.shape}"
        
        print("âœ… HoME Meta Forward æµ‹è¯•é€šè¿‡!")
        
        return output
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥HoME kernels: {e}")
        return None
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return None

def test_home_expert_forward():
    """æµ‹è¯•HoME Expert Forwardæ¥å£ï¼ˆå•å±‚ï¼‰"""
    print("\n=== HoME Expert Forward æµ‹è¯• ===")
    
    # å‚æ•°è®¾ç½®
    batch_size = 4096
    input_dim = 512
    output_dim = 1024
    num_experts = 8
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    input_tensor = torch.randn(batch_size, input_dim, dtype=torch.bfloat16, device=device)
    expert_weights = torch.randn(num_experts, input_dim, output_dim, dtype=torch.bfloat16, device=device)
    expert_biases = torch.randn(num_experts, output_dim, dtype=torch.bfloat16, device=device)
    
    try:
        import sys
        sys.path.append('/root/home_moe_kernels/build')
        import home_kernels
        
        # é¢„çƒ­
        for _ in range(5):
            _ = home_kernels.home_expert_forward_bf16(
                input_tensor, expert_weights, expert_biases,
                num_experts, True
            )
        
        # ä½¿ç”¨CUDA eventsè¿›è¡Œç²¾ç¡®è®¡æ—¶
        torch.cuda.synchronize()
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)
        
        start_event.record()
        for _ in range(50):
            output = home_kernels.home_expert_forward_bf16(
                input_tensor, expert_weights, expert_biases,
                num_experts, True
            )
        end_event.record()
        
        torch.cuda.synchronize()
        avg_time = start_event.elapsed_time(end_event) / 50  # å·²ç»æ˜¯æ¯«ç§’
        
        print(f"HoME Expert Forward å¹³å‡æ—¶é—´: {avg_time:.4f} ms")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"è¾“å‡ºæ•°æ®ç±»å‹: {output.dtype}")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (batch_size, num_experts, output_dim)
        assert output.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: æœŸæœ› {expected_shape}, å®é™… {output.shape}"
        
        print("âœ… HoME Expert Forward æµ‹è¯•é€šè¿‡!")
        
        return output
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥HoME kernels: {e}")
        return None
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    print("å¼€å§‹HoME kernelsæµ‹è¯•...")
    
    # æµ‹è¯•PyTorchå®ç°
    pytorch_output, pytorch_model = test_pytorch_meta_forward()
    
    # æµ‹è¯•CUDA kernelå®ç°
    cuda_output = test_home_meta_forward()
    
    # æµ‹è¯•å•å±‚ä¸“å®¶ç½‘ç»œ
    expert_output = test_home_expert_forward()
    
    # ç²¾åº¦å¯¹æ¯”æµ‹è¯•
    
    # æ€§èƒ½å¯¹æ¯”æ€»ç»“
    if pytorch_output is not None and cuda_output is not None:
        print("\n=== æ€§èƒ½å¯¹æ¯”æ€»ç»“ ===")
        print("PyTorchå®ç°å’ŒCUDA kernelå®ç°éƒ½å·²å®Œæˆæµ‹è¯•")
        print("è¯¦ç»†æ€§èƒ½æ•°æ®è¯·æŸ¥çœ‹ä¸Šè¿°æµ‹è¯•ç»“æœ")
    
    if expert_output is not None:
        print("\nğŸ‰ HoME Expert Forwardæµ‹è¯•é€šè¿‡! kernelsé›†æˆæˆåŠŸ!")
        if cuda_output is not None:
            print("ğŸ‰ HoME Meta Forwardæµ‹è¯•ä¹Ÿé€šè¿‡!")
        else:
            print("âš ï¸  HoME Meta Forwardå‡½æ•°æš‚æœªæ­£ç¡®å¯¼å‡ºï¼Œä½†Expert ForwardåŠŸèƒ½æ­£å¸¸")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
